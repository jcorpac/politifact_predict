{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolitiFact Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/jcorpac/politifact_predict/blob/master/PolitiFact%20Prediction.ipynb",
      "authorship_tag": "ABX9TyMVrgswiaB1bbbL+umKIfIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcorpac/politifact_predict/blob/master/PolitiFact%20Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zywNdITjiQtK",
        "colab_type": "text"
      },
      "source": [
        "# Library Imports and metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-ZiH75FeONP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from os import path\n",
        "import requests\n",
        "import hashlib\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSo1gL-EAflY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "import gspread_dataframe as gs_df\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "project_id = 'jcorpac'\n",
        "bucket_name = 'politifact_prediction'\n",
        "\n",
        "g_sheets_key = \"1e0c0Cuv6yIg8Z30vfF8lvHVX9yv6y-ADu56In8ZZUk0\"\n",
        "g_sheets_raw_data_tag = \"raw_data\"\n",
        "g_sheets_training_data_tag = \"training_data\"\n",
        "\n",
        "bucket_raw_data = \"data_set/politifact_data.csv\"\n",
        "bucket_training_data = \"data_set/training_data.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HpXCwpnw3U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOTAL_PAGES = 566\n",
        "col_index = ['name', 'quote_desc', 'quote', 'link', 'date_line', 'rating', 'sha256']\n",
        "local_raw_data = \"./politifact_data.csv\"\n",
        "local_training_data = \"./training_data.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NsAYACkyaAc",
        "colab_type": "text"
      },
      "source": [
        "# **Data Scraping Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNgZEsBcicL0",
        "colab_type": "text"
      },
      "source": [
        "## get_politifacts_page()\n",
        "\n",
        "---\n",
        "### Parameters\n",
        "* page_num - The page from Politifact's fact-check list for the function to \n",
        "receive. Politifact orders its search from newest to oldest. The first page or two should be sufficient for updating an existing dataset.\n",
        "\n",
        "---\n",
        "### Returns\n",
        "A Pandas DataFrame containing up to 30 rows (determined by the Politifact page). The columns in the dataframe indicate the following.\n",
        "\n",
        "* name - The name of the person who made the quote.\n",
        "* quote_desc - The place and date where the quote was made.\n",
        "* quote - The actual quote. Sometimes preceeded with connecting phrases such as \"says\" or \"tweeted\". These need to be cleared out in later data processing.\n",
        "* link - The URL for the associated Politifact article.\n",
        "* date_line - The name of the Politifact reporter and the date that the entry was made. Best place to parse out a date if sorting the data by date.\n",
        "* rating - The rating that Politifact assigned for a given quote.\n",
        "* sha256 - A unique hexidecimal identifier generated from the above columns. Can be used for duplicate entry detection and repeatable train/validation/test dataset splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHg-uv6BmxC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_politifacts_page(page_num):\n",
        "    page = requests.get(f\"https://www.politifact.com/factchecks/list/?page={page_num}\")\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    page_data = pd.DataFrame(columns=col_index)\n",
        "    items = soup.findAll('article', class_=\"m-statement\")\n",
        "    for item in items:\n",
        "        # Who (or what) is the quote attributed to\n",
        "        name = item.find('a', class_=\"m-statement__name\").get_text().strip()\n",
        "        # Where was the quote made?\n",
        "        quote_desc = item.find('div', class_=\"m-statement__desc\").get_text().strip()\n",
        "        # What is the quote\n",
        "        quote = item.find('div', class_=\"m-statement__quote\").find('a')\n",
        "        link = f\"https://politifact.com{quote['href'].strip()}\"\n",
        "        quote_text = quote.get_text().strip()\n",
        "        # Date line with attribution, used for SHA-256 signature\n",
        "        date_line = item.find('footer', class_=\"m-statement__footer\").get_text().strip()\n",
        "        # Rating - Label to be predicted. Might filter some of these out later\n",
        "        rating = item.find('div', class_=\"m-statement__meter\").find('picture').find('img')['alt']\n",
        "\n",
        "        # SHA 256 hash used for identifying duplicate entries.\n",
        "        # Will also be converted to an int for repeatable train/validation/test splits. \n",
        "        sha256 = hashlib.sha256(f\"{name}{quote_desc}{quote_text}{link}{date_line}{rating}\".encode()).hexdigest()\n",
        "\n",
        "        new_row = {col_index[0]: name, col_index[1]: quote_desc, col_index[2]: quote_text, col_index[3]: link,\n",
        "                   col_index[4]: date_line, col_index[5]: rating, col_index[6]: sha256}\n",
        "        page_data = page_data.append(new_row, ignore_index=True)\n",
        "    return page_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3keEbz7plooI",
        "colab_type": "text"
      },
      "source": [
        "## update_politifact_data_set()\n",
        "\n",
        "---\n",
        "### Suggested uses\n",
        "\n",
        "*   `update_politifact_data_set()` - Creates a new data frame with the 30 most recent updates. Good for creating short sample test datasets.\n",
        "*   `update_politifact_data_set(data_set=data_frame)` - Updates an existing data frame with 1 page of the most recent updates. Useful for daily updates.\n",
        "*   `update_politifact_data_set(3, data_set=data_frame)` - Updates an existing data frame with 3 pages of the most recent updates. Increase the number to retrieve more pages if the data hasn't been updated for awhile.\n",
        "*    `update_politifact_data_set(TOTAL_PAGES)` - Scrapes the whole Politifact index. Make sure to update the TOTAL_PAGES variable first to indicate the last page on the site. \n",
        "\n",
        "---\n",
        "### Parameters\n",
        "* end_page - The index of the last page in the retrieval process. Default set to 1 to update from the latest 30 entries.\n",
        "* start_page - The index of the first page in the retrival process. Leave this field blank to start from the most recent entries.\n",
        "* data_set - A Pandas Dataframe, the data set to be updated. If this field is blank, or None, a new DataFrame will be generated.\n",
        "\n",
        "---\n",
        "### Returns\n",
        "A collection of pages in the format of the get_politifacts_page() function listed above, concatenated into one Pandas DataFrame. This data will have the existing data_set appended to it with the most recently posted data first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMaRMc9XKtHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_politifact_data_set(end_page=1, start_page=1, data_set=None):\n",
        "    if data_set is None:\n",
        "        data_set = pd.DataFrame(columns=col_index)\n",
        "\n",
        "    if end_page > TOTAL_PAGES:\n",
        "        end_page = TOTAL_PAGES\n",
        "\n",
        "    for page_number in tqdm(range(end_page, start_page-1, -1)):\n",
        "        data_set = get_politifacts_page(page_num=page_number).append(data_set, ignore_index=True)\n",
        "        # 3 second delay between page requests. Don't be rude and slam their server.\n",
        "        time.sleep(3)\n",
        "\n",
        "    # Remove any duplicate entries\n",
        "    data_set = data_set.drop_duplicates(subset=\"sha256\")\n",
        "\n",
        "    return data_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnCV7Ew1zDae",
        "colab_type": "text"
      },
      "source": [
        "# Load Politifact data from CSV or Google Sheets and Update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnbSvvVzLGcV",
        "colab_type": "text"
      },
      "source": [
        "### Retrieve current csv data file from GCS Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMFpa9eJKO4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "from apiclient.http import MediaIoBaseDownload\n",
        "gcs_service = build('storage', 'v1')\n",
        "\n",
        "with open(local_raw_data, 'wb') as f:\n",
        "  # Download the file from a given Google Cloud Storage bucket.\n",
        "  request = gcs_service.objects().get_media(bucket=bucket_name,\n",
        "                                            object=bucket_raw_data)\n",
        "  media = MediaIoBaseDownload(f, request)\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    _, done = media.next_chunk()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG0zkHrSBcPv",
        "colab_type": "text"
      },
      "source": [
        "### Read data from csv data file and update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EffL5hU-ch_",
        "colab_type": "code",
        "outputId": "a1641199-9946-4c55-e2c8-e1f36d042610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if path.exists(local_raw_data):\n",
        "    data = pd.read_csv(local_raw_data, sep='|')\n",
        "else:\n",
        "    data = None\n",
        "\n",
        "data = update_politifact_data_set(data_set=data)\n",
        "\n",
        "data.to_csv(local_raw_data, header=True, index=False, sep='|')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:03<00:00,  3.82s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxr7zwKtIAzP",
        "colab_type": "text"
      },
      "source": [
        "### Upload CSV data to Google Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UGAhHtjEAe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "media = MediaFileUpload(local_raw_data, \n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "\n",
        "request = gcs_service.objects().insert(bucket=bucket_name, \n",
        "                                       name=bucket_raw_data,\n",
        "                                       media_body=media)\n",
        "\n",
        "response = None\n",
        "while response is None:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, response = request.next_chunk()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL5GLVxyBo-u",
        "colab_type": "text"
      },
      "source": [
        "### Read data from Google Sheets and update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfGDX_Ys-zQ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e34c96b-cccc-4bbe-bad2-d1672752c3b5"
      },
      "source": [
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "sheet = gc.open_by_key(g_sheets_key).worksheet(g_sheets_raw_data_tag)\n",
        "data = gs_df.get_as_dataframe(sheet)\n",
        "data = update_politifact_data_set(data_set=data)\n",
        "gs_df.set_with_dataframe(sheet, data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1oVaLCdZyvn",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6KCz9NZBxbm",
        "colab_type": "text"
      },
      "source": [
        "### Copy raw data set and extract model-relevant features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43cp3qWdAqGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data_set = data.copy()\n",
        "# Convert SHA256 value from hex string to integer.\n",
        "new_data_set.sha256 = new_data_set.sha256.apply(int, base=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkgcfVyCsXkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "politifact_data = new_data_set[[\"quote\", \"rating\", \"sha256\"]]\n",
        "\n",
        "# Rows with Flip-related labels are not relevant to the model.\n",
        "politifact_data = politifact_data[~politifact_data.rating.isin([\"full-flop\", \"half-flip\", \"no-flip\"])]\n",
        "\n",
        "# Cast ratings as strings to avoid errors when changing case\n",
        "politifact_data.rating = politifact_data.rating.astype(str)\n",
        "\n",
        "# Remove case from the rating to merge False/false and True/true ratings\n",
        "politifact_data.rating = politifact_data.rating.str.lower()\n",
        "\n",
        "# If the last digit in the converted SHA value is 0-7, label it for training data\n",
        "# If the last digit is 8 or 9, label it for the test set\n",
        "politifact_data[\"is_test\"] = politifact_data.sha256 % 10 >= 8\n",
        "\n",
        "# Once we have the split, we don't need the SHA value anymore\n",
        "politifact_data.drop(columns=\"sha256\", inplace=True)\n",
        "\n",
        "# Some duplicate quotes remain, remove them.\n",
        "politifact_data.drop_duplicates(subset=\"quote\", inplace=True)\n",
        "\n",
        "# Remove connecting phrases from quotes unlikely to appear when model is in use.\n",
        "connecting_phrases = [\"Says \", \"Say \", \"Tweeted \", \"Quoted \", \"Quotes \", \"Says of \"]\n",
        "for phrase in connecting_phrases:\n",
        "    politifact_data.quote = politifact_data.quote.str.replace(phrase, \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9_aq0uMMtgj",
        "colab_type": "text"
      },
      "source": [
        "### Update training data in Google Sheets spreadsheet and GCS Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iL-X9BOvgoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sheet = gc.open_by_key(g_sheets_key).worksheet(g_sheets_training_data_tag)\n",
        "gs_df.set_with_dataframe(training_sheet, politifact_data)\n",
        "\n",
        "politifact_data.to_csv(local_training_data, header=True, index=False, sep='|')\n",
        "media = MediaFileUpload(local_training_data, \n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "\n",
        "request = gcs_service.objects().insert(bucket=bucket_name, \n",
        "                                       name=bucket_training_data,\n",
        "                                       media_body=media)\n",
        "\n",
        "response = None\n",
        "while response is None:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, response = request.next_chunk()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-Vdg-QM8Ya",
        "colab_type": "text"
      },
      "source": [
        "### Split training/test data and convert to numpy arrays for TensorFlow models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlutF5ssC354",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data, test_data = politifact_data[~politifact_data.is_test], politifact_data[politifact_data.is_test]\n",
        "\n",
        "training_features = training_data.quote.to_numpy()\n",
        "training_labels = training_data.rating\n",
        "training_labels_one_hot = pd.get_dummies(training_labels)\n",
        "ratings = training_labels_one_hot.columns.to_list()\n",
        "training_labels_one_hot = training_labels_one_hot.to_numpy()\n",
        "\n",
        "test_features = test_data.quote.to_numpy()\n",
        "test_labels = test_data.rating\n",
        "test_labels_one_hot = pd.get_dummies(test_labels).to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDrwm7JK3xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting ratings into numeric scores. May be used for regression model later.\n",
        "rating_scores = {'pants-fire':0.0, 'false':0.2, 'barely-true':0.4, 'half-true':0.6, 'mostly-true':0.8, 'true':1.0}\n",
        "\n",
        "training_labels_scores = training_labels.map(rating_scores).to_numpy()\n",
        "test_labels_scores = test_labels.map(rating_scores).to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtVY6LSTNRxq",
        "colab_type": "text"
      },
      "source": [
        "# Experimental Code (WIP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_SNrrLGNVtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}